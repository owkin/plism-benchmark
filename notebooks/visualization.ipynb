{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from plismbench.utils.metrics import (\n",
    "    format_results,\n",
    "    get_aggregated_results,\n",
    "    get_leaderboard_results,\n",
    ")\n",
    "from plismbench.utils.viz import EXTRACTOR_LABELS_DICT, display_plism_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`metrics_root_dir` should have this architecture (as produced by `plismbench evaluate`).\n",
    "By default, metrics are only computed for 8139 tiles. As as example, you might have different folders corresponding to different extractors (here `h0_mini` and `conch`).\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── 2713_tiles\n",
    "│   ├── conch\n",
    "│   │   ├── metrics.csv\n",
    "│   │   ├── pickles\n",
    "│   │   └── results.csv\n",
    "│   ├── h0_mini\n",
    "│   │   ├── metrics.csv\n",
    "│   │   ├── pickles\n",
    "│   │   └── results.csv\n",
    "...\n",
    "├── 5426_tiles\n",
    "│   ├── conch\n",
    "│   │   ├── metrics.csv\n",
    "│   │   ├── pickles\n",
    "│   │   └── results.csv\n",
    "│   ├── h0_mini\n",
    "│   │   ├── metrics.csv\n",
    "│   │   ├── pickles\n",
    "│   │   └── results.csv\n",
    "...\n",
    "└── 8139_tiles\n",
    "    ├── conch\n",
    "    │   ├── metrics.csv\n",
    "    │   ├── pickles\n",
    "    │   └── results.csv\n",
    "    ├── h0_mini\n",
    "    │   ├── metrics.csv\n",
    "    │   ├── pickles\n",
    "    └── └── results.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set metrics root directory\n",
    "metrics_root_dir = Path(\"/home/owkin/project/plism_metrics/\")\n",
    "\n",
    "# Retrieve a more detailed version of the results\n",
    "agg_type = \"median\"  # choose between \"median\" or \"mean\"\n",
    "n_tiles = 8139  # default number of tiles\n",
    "raw_results = format_results(\n",
    "    metrics_root_dir,\n",
    "    agg_type=agg_type,\n",
    "    n_tiles=n_tiles,\n",
    ")\n",
    "\n",
    "raw_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and rank results for a specific metric, aggregation over pairs and robustness type\n",
    "metric = \"cosine_similarity\"  # choose between \"cosine_similarity\", \"top_1_accuracy\", \"top_3_accuracy\", \"top_5_accuracy\", \"top_10_accuracy\"\n",
    "robustness_type = \"all\"  # choose between \"all\", \"inter-scanner\", \"inter-staining\", \"inter-scanner, inter-staining\"\n",
    "results = get_aggregated_results(\n",
    "    results=raw_results,\n",
    "    metric_name=metric,\n",
    "    agg_type=agg_type,\n",
    "    robustness_type=robustness_type,\n",
    ")\n",
    "\n",
    "results.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "display_plism_metrics(\n",
    "    raw_results,\n",
    "    xlim=(-0.005, 0.25),  # may depend on the metric displayed on x-axis\n",
    "    ylim=(0.4, 0.9),  # may depend on the metric displayed on y-axis\n",
    "    metric_x=\"top_10_accuracy_median\",  # should be in ``raw_results``\n",
    "    metric_y=\"cosine_similarity_median\",  # should be in ``raw_results``\n",
    "    robustness_x=\"all\",  # should be in ``raw_results``\n",
    "    robustness_y=\"all\",  # should be in ``raw_results``\n",
    "    label_x=\"Top-10 accuracy (all pairs)\",\n",
    "    label_y=\"Cosine similarity (all pairs)\",\n",
    "    fig_save_path=None,  # can be None, a string or Path. You can export to .svg then use Inkscape to move the overlapping labels apart.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leaderboard results\n",
    "leaderboard = get_leaderboard_results(metrics_root_dir=metrics_root_dir)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to markdown\n",
    "leaderboard.index = leaderboard.index.map(EXTRACTOR_LABELS_DICT)\n",
    "print(leaderboard.astype(str).to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vesibench)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdedc4665527ff43f21f83597a20c9857360c358c9dc57bfea9e7d2a253a1bcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
